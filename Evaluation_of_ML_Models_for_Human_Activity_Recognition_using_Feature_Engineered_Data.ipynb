{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1v4SACmqoWKm2zAXsdIWJyoZJdyJLzVF3",
      "authorship_tag": "ABX9TyO6UGdjSxalr8tTh1K77ipL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SenolIsci/ML-Model-Starters/blob/main/Evaluation_of_ML_Models_for_Human_Activity_Recognition_using_Feature_Engineered_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate Machine Learning Algorithms for Human Activity Recognition using Feature Engineered Data\n",
        "* Activity Recognition Using Smartphones Dataset\n",
        "* Develop 1D Convolutional Neural Network\n",
        "* Tuned 1D Convolutional Neural Network\n",
        "* Multi-Headed 1D Convolutional Neural Network"
      ],
      "metadata": {
        "id": "XECOwZWscaKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional Neural Network Models for Human Activity Recognition\n",
        "\n",
        "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
        "\n",
        "Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field.\n",
        "\n",
        "Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data.\n",
        "\n",
        "A standard human activity recognition dataset is the ‘Activity Recognition Using Smart Phones Dataset’ made available in 2012.\n",
        "\n",
        "The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository:\n",
        "\n",
        "[Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)\n",
        "\n",
        "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos.\n",
        "\n",
        "1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "by Jason Brownlee on September 21, 2018 in Deep Learning for Time Series\n",
        "Tweet Tweet  Share\n",
        "Last Updated on August 28, 2020\n",
        "\n",
        "Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements.\n",
        "\n",
        "Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field.\n",
        "\n",
        "Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data.\n",
        "\n",
        "In this tutorial, you will discover how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "How to load and prepare the data for a standard human activity recognition dataset and develop a single 1D CNN model that achieves excellent performance on the raw data.\n",
        "How to further tune the performance of the model, including data transformation, filter maps, and kernel sizes.\n",
        "How to develop a sophisticated multi-headed one-dimensional convolutional neural network model that provides an ensemble-like result.\n",
        "Kick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n",
        "\n",
        "Let’s get started.\n",
        "\n",
        "How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "Photo by Wolfgang Staudt, some rights reserved.\n",
        "\n",
        "Tutorial Overview\n",
        "This tutorial is divided into four parts; they are:\n",
        "\n",
        "Activity Recognition Using Smartphones Dataset\n",
        "Develop 1D Convolutional Neural Network\n",
        "Tuned 1D Convolutional Neural Network\n",
        "Multi-Headed 1D Convolutional Neural Network\n",
        "\n",
        "Activity Recognition Using Smartphones Dataset\n",
        "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
        "\n",
        "A standard human activity recognition dataset is the ‘Activity Recognition Using Smart Phones Dataset’ made available in 2012.\n",
        "\n",
        "It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper “A Public Domain Dataset for Human Activity Recognition Using Smartphones.” The dataset was modeled with machine learning algorithms in their 2012 paper titled “Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.”\n",
        "\n",
        "The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository:\n",
        "\n",
        "Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository\n",
        "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos.\n",
        "\n",
        "Below is an example video of a subject performing the activities while their movement data is being recorded.\n",
        "\n",
        "\n",
        "The six activities performed were as follows:\n",
        "\n",
        "Walking\n",
        "Walking Upstairs\n",
        "Walking Downstairs\n",
        "Sitting\n",
        "Standing\n",
        "Laying\n",
        "The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side.\n",
        "\n",
        "The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included:\n",
        "\n",
        "Pre-processing accelerometer and gyroscope using noise filters.\n",
        "Splitting data into fixed windows of 2.56 seconds (128 data points) with 50% overlap.\n",
        "Splitting of accelerometer data into gravitational (total) and body motion components.\n",
        "Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available.\n",
        "\n",
        "A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features.\n",
        "\n",
        "The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test.\n",
        "\n",
        "There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step.\n",
        "\n",
        "Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section.\n",
        "\n",
        "The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. \n",
        "\n",
        "To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file.\n"
      ],
      "metadata": {
        "id": "UQKFCcEkc-A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling Feature Engineered Data"
      ],
      "metadata": {
        "id": "NFs1toHxhIt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Models\n",
        "Next, we can define a list of machine learning models to evaluate on this problem.\n",
        "\n",
        "We will evaluate the models using default configurations. We are not looking for optimal configurations of these models at this point, just a general idea of how well sophisticated models with default configurations perform on this problem.\n",
        "\n",
        "We will evaluate a diverse set of nonlinear and ensemble machine learning algorithms, specifically:\n",
        "\n",
        "Nonlinear Algorithms:\n",
        "\n",
        "k-Nearest Neighbors\n",
        "Classification and Regression Tree\n",
        "Support Vector Machine\n",
        "Naive Bayes\n",
        "Ensemble Algorithms:\n",
        "\n",
        "Bagged Decision Trees\n",
        "Random Forest\n",
        "Extra Trees\n",
        "Gradient Boosting Machine"
      ],
      "metadata": {
        "id": "uqajzXmMUcOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will develop code to load the feature-engineered version of the dataset and evaluate a suite of nonlinear machine learning algorithms, including SVM used in the original paper.\n",
        "\n",
        "\n",
        "\n",
        "The results of methods using the feature-engineered version of the dataset provide a baseline for any methods developed for the raw data version.\n",
        "\n",
        "The first step is to load the train and test input (X) and output (y) data.\n",
        "\n",
        "Specifically, the following files:\n",
        "\n",
        "* HARDataset/train/X_train.txt\n",
        "* HARDataset/train/y_train.txt\n",
        "* HARDataset/test/X_test.txt\n",
        "* HARDataset/test/y_test.txt"
      ],
      "metadata": {
        "id": "e6SZOYF0hbqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spot check on engineered-features\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\t# load input data\n",
        "\tX = load_file(prefix + group + '/X_'+group+'.txt')\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# flatten y\n",
        "\ttrainy, testy = trainy[:,0], testy[:,0]\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# create a dict of standard models to evaluate {name:object}\n",
        "def define_models(models=dict()):\n",
        "\t# nonlinear models\n",
        "\tmodels['knn'] = KNeighborsClassifier(n_neighbors=7)\n",
        "\tmodels['cart'] = DecisionTreeClassifier()\n",
        "\tmodels['svm'] = SVC()\n",
        "\tmodels['bayes'] = GaussianNB()\n",
        "\t# ensemble models\n",
        "\tmodels['bag'] = BaggingClassifier(n_estimators=100)\n",
        "\tmodels['rf'] = RandomForestClassifier(n_estimators=100)\n",
        "\tmodels['et'] = ExtraTreesClassifier(n_estimators=100)\n",
        "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
        "\tprint('Defined %d models' % len(models))\n",
        "\treturn models\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(trainX, trainy, testX, testy, model):\n",
        "\t# fit the model\n",
        "\tmodel.fit(trainX, trainy)\n",
        "\t# make predictions\n",
        "\tyhat = model.predict(testX)\n",
        "\t# evaluate predictions\n",
        "\taccuracy = accuracy_score(testy, yhat)\n",
        "\treturn accuracy * 100.0\n",
        "\n",
        "# evaluate a dict of models {name:object}, returns {name:score}\n",
        "def evaluate_models(trainX, trainy, testX, testy, models):\n",
        "\tresults = dict()\n",
        "\tfor name, model in models.items():\n",
        "\t\t# evaluate the model\n",
        "\t\tresults[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
        "\t\t# show process\n",
        "\t\tprint('>%s: %.3f' % (name, results[name]))\n",
        "\treturn results\n",
        "\n",
        "# print and plot the results\n",
        "def summarize_results(results, maximize=True):\n",
        "\t# create a list of (name, mean(scores)) tuples\n",
        "\tmean_scores = [(k,v) for k,v in results.items()]\n",
        "\t# sort tuples by mean score\n",
        "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
        "\t# reverse for descending order (e.g. for accuracy)\n",
        "\tif maximize:\n",
        "\t\tmean_scores = list(reversed(mean_scores))\n",
        "\tprint()\n",
        "\tfor name, score in mean_scores:\n",
        "\t\tprint('Name=%s, Score=%.3f' % (name, score))\n",
        "\n",
        "# load dataset\n",
        "trainX, trainy, testX, testy = load_dataset()\n",
        "# get model list\n",
        "models = define_models()\n",
        "# evaluate models\n",
        "results = evaluate_models(trainX, trainy, testX, testy, models)\n",
        "# summarize results\n",
        "summarize_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEJJKokAUvmG",
        "outputId": "dcf1c19d-5e8e-4ca6-b623-f9287fef1132"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 561) (7352, 1)\n",
            "(2947, 561) (2947, 1)\n",
            "(7352, 561) (7352,) (2947, 561) (2947,)\n",
            "Defined 8 models\n",
            ">knn: 90.329\n",
            ">cart: 86.359\n",
            ">svm: 95.046\n",
            ">bayes: 77.027\n",
            ">bag: 89.786\n",
            ">rf: 92.297\n",
            ">et: 93.926\n",
            ">gbm: 93.824\n",
            "\n",
            "Name=svm, Score=95.046\n",
            "Name=et, Score=93.926\n",
            "Name=gbm, Score=93.824\n",
            "Name=rf, Score=92.297\n",
            "Name=knn, Score=90.329\n",
            "Name=bag, Score=89.786\n",
            "Name=cart, Score=86.359\n",
            "Name=bayes, Score=77.027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOsQ6UOsVWj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Standardization/Scaling\n",
        "\n",
        "Standardization/Scaling is optional"
      ],
      "metadata": {
        "id": "HGXmGyJUhQRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n",
        "# standardize data\n",
        "def scale_data(trainX, testX, standardize=True):\n",
        "\t\"\"\"\n",
        "\tusage: \ttrainX, testX = scale_data(trainX, testX, True)\n",
        "\t\"\"\"\n",
        "\n",
        "\tif standardize:\n",
        "\t\ts = StandardScaler()\n",
        "\t\t# fit on training data\n",
        "\t\ts.fit(trainX)\n",
        "\t\t# apply to training and test data\n",
        "\n",
        "\t\tflatTrainX = s.transform(trainX)\n",
        "\t\tflatTestX = s.transform(testX)\n",
        "\n",
        "\treturn flatTrainX, flatTestX\n"
      ],
      "metadata": {
        "id": "HuYf5OVterFp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WITH SCALED DATA"
      ],
      "metadata": {
        "id": "rv0tmeV6X_8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spot check on engineered-features\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\t# load input data\n",
        "\tX = load_file(prefix + group + '/X_'+group+'.txt')\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# flatten y\n",
        "\ttrainy, testy = trainy[:,0], testy[:,0]\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# create a dict of standard models to evaluate {name:object}\n",
        "def define_models(models=dict()):\n",
        "\t# nonlinear models\n",
        "\tmodels['knn'] = KNeighborsClassifier(n_neighbors=7)\n",
        "\tmodels['cart'] = DecisionTreeClassifier()\n",
        "\tmodels['svm'] = SVC()\n",
        "\tmodels['bayes'] = GaussianNB()\n",
        "\t# ensemble models\n",
        "\tmodels['bag'] = BaggingClassifier(n_estimators=100)\n",
        "\tmodels['rf'] = RandomForestClassifier(n_estimators=100)\n",
        "\tmodels['et'] = ExtraTreesClassifier(n_estimators=100)\n",
        "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=100)\n",
        "\tprint('Defined %d models' % len(models))\n",
        "\treturn models\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(trainX, trainy, testX, testy, model):\n",
        "\t# fit the model\n",
        "\tmodel.fit(trainX, trainy)\n",
        "\t# make predictions\n",
        "\tyhat = model.predict(testX)\n",
        "\t# evaluate predictions\n",
        "\taccuracy = accuracy_score(testy, yhat)\n",
        "\treturn accuracy * 100.0\n",
        "\n",
        "# evaluate a dict of models {name:object}, returns {name:score}\n",
        "def evaluate_models(trainX, trainy, testX, testy, models):\n",
        "\tresults = dict()\n",
        "\tfor name, model in models.items():\n",
        "\t\t# evaluate the model\n",
        "\t\tresults[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
        "\t\t# show process\n",
        "\t\tprint('>%s: %.3f' % (name, results[name]))\n",
        "\treturn results\n",
        "\n",
        "# print and plot the results\n",
        "def summarize_results(results, maximize=True):\n",
        "\t# create a list of (name, mean(scores)) tuples\n",
        "\tmean_scores = [(k,v) for k,v in results.items()]\n",
        "\t# sort tuples by mean score\n",
        "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
        "\t# reverse for descending order (e.g. for accuracy)\n",
        "\tif maximize:\n",
        "\t\tmean_scores = list(reversed(mean_scores))\n",
        "\tprint()\n",
        "\tfor name, score in mean_scores:\n",
        "\t\tprint('Name=%s, Score=%.3f' % (name, score))\n",
        "\n",
        "# load dataset\n",
        "trainX, trainy, testX, testy = load_dataset()\n",
        "\n",
        "# scale data\n",
        "\n",
        "trainX, testX = scale_data(trainX, testX, True)\n",
        "\n",
        "# get model list\n",
        "models = define_models()\n",
        "# evaluate models\n",
        "results = evaluate_models(trainX, trainy, testX, testy, models)\n",
        "# summarize results\n",
        "summarize_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70993378-87ac-499d-8421-f0a228e386a4",
        "id": "dnHVssjuYBVv"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 561) (7352, 1)\n",
            "(2947, 561) (2947, 1)\n",
            "(7352, 561) (7352,) (2947, 561) (2947,)\n",
            "Defined 8 models\n",
            ">knn: 88.633\n",
            ">cart: 85.918\n",
            ">svm: 95.215\n",
            ">bayes: 77.027\n",
            ">bag: 90.024\n",
            ">rf: 92.263\n",
            ">et: 93.688\n",
            ">gbm: 93.960\n",
            "\n",
            "Name=svm, Score=95.215\n",
            "Name=gbm, Score=93.960\n",
            "Name=et, Score=93.688\n",
            "Name=rf, Score=92.263\n",
            "Name=bag, Score=90.024\n",
            "Name=knn, Score=88.633\n",
            "Name=cart, Score=85.918\n",
            "Name=bayes, Score=77.027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "51K4ASlmaClI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}