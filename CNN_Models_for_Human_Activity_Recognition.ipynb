{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1v4SACmqoWKm2zAXsdIWJyoZJdyJLzVF3",
      "authorship_tag": "ABX9TyNgAdRmnqNRBVQPvL/KyNDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SenolIsci/ML-Model-Starters/blob/main/CNN_Models_for_Human_Activity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convolutional Neural Network Models for Human Activity Recognition\n",
        "* Activity Recognition Using Smartphones Dataset\n",
        "* Develop 1D Convolutional Neural Network\n",
        "* Tuned 1D Convolutional Neural Network\n",
        "* Multi-Headed 1D Convolutional Neural Network"
      ],
      "metadata": {
        "id": "XECOwZWscaKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional Neural Network Models for Human Activity Recognition\n",
        "\n",
        "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
        "\n",
        "Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field.\n",
        "\n",
        "Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data.\n",
        "\n",
        "A standard human activity recognition dataset is the ‘Activity Recognition Using Smart Phones Dataset’ made available in 2012.\n",
        "\n",
        "The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository:\n",
        "\n",
        "[Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)\n",
        "\n",
        "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos.\n",
        "\n",
        "1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "by Jason Brownlee on September 21, 2018 in Deep Learning for Time Series\n",
        "Tweet Tweet  Share\n",
        "Last Updated on August 28, 2020\n",
        "\n",
        "Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements.\n",
        "\n",
        "Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field.\n",
        "\n",
        "Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data.\n",
        "\n",
        "In this tutorial, you will discover how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "How to load and prepare the data for a standard human activity recognition dataset and develop a single 1D CNN model that achieves excellent performance on the raw data.\n",
        "How to further tune the performance of the model, including data transformation, filter maps, and kernel sizes.\n",
        "How to develop a sophisticated multi-headed one-dimensional convolutional neural network model that provides an ensemble-like result.\n",
        "Kick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n",
        "\n",
        "Let’s get started.\n",
        "\n",
        "How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "Photo by Wolfgang Staudt, some rights reserved.\n",
        "\n",
        "Tutorial Overview\n",
        "This tutorial is divided into four parts; they are:\n",
        "\n",
        "Activity Recognition Using Smartphones Dataset\n",
        "Develop 1D Convolutional Neural Network\n",
        "Tuned 1D Convolutional Neural Network\n",
        "Multi-Headed 1D Convolutional Neural Network\n",
        "\n",
        "Activity Recognition Using Smartphones Dataset\n",
        "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
        "\n",
        "A standard human activity recognition dataset is the ‘Activity Recognition Using Smart Phones Dataset’ made available in 2012.\n",
        "\n",
        "It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper “A Public Domain Dataset for Human Activity Recognition Using Smartphones.” The dataset was modeled with machine learning algorithms in their 2012 paper titled “Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.”\n",
        "\n",
        "The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository:\n",
        "\n",
        "Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository\n",
        "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos.\n",
        "\n",
        "Below is an example video of a subject performing the activities while their movement data is being recorded.\n",
        "\n",
        "\n",
        "The six activities performed were as follows:\n",
        "\n",
        "Walking\n",
        "Walking Upstairs\n",
        "Walking Downstairs\n",
        "Sitting\n",
        "Standing\n",
        "Laying\n",
        "The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side.\n",
        "\n",
        "The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included:\n",
        "\n",
        "Pre-processing accelerometer and gyroscope using noise filters.\n",
        "Splitting data into fixed windows of 2.56 seconds (128 data points) with 50% overlap.\n",
        "Splitting of accelerometer data into gravitational (total) and body motion components.\n",
        "Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available.\n",
        "\n",
        "A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features.\n",
        "\n",
        "The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test.\n",
        "\n",
        "There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step.\n",
        "\n",
        "Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section.\n",
        "\n",
        "The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. \n",
        "\n",
        "To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file.\n"
      ],
      "metadata": {
        "id": "UQKFCcEkc-A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Develop 1D Convolutional Neural Network"
      ],
      "metadata": {
        "id": "aTUZD-R5gQxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\tverbose, epochs, batch_size = 0, 10, 32\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(MaxPooling1D(pool_size=2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=1):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# repeat experiment\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
        "\t\tscore = score * 100.0\n",
        "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\t\tscores.append(score)\n",
        "\t# summarize results\n",
        "\tsummarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ey1GCqJY9k",
        "outputId": "944c1a7d-ee1a-4b55-c6a0-e82827659b28"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">#1: 91.076\n",
            "[91.07567071914673]\n",
            "Accuracy: 91.076% (+/-0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AsHcS2xkhJG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuned 1D Convolutional Neural Network"
      ],
      "metadata": {
        "id": "NFs1toHxhIt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Standardization/Scaling"
      ],
      "metadata": {
        "id": "HGXmGyJUhQRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn model with standardization\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# standardize data\n",
        "def scale_data(trainX, testX, standardize):\n",
        "\t# remove overlap\n",
        "\tcut = int(trainX.shape[1] / 2)\n",
        "\tlongX = trainX[:, -cut:, :]\n",
        "\t# flatten windows\n",
        "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
        "\t# flatten train and test\n",
        "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
        "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
        "\t# standardize\n",
        "\tif standardize:\n",
        "\t\ts = StandardScaler()\n",
        "\t\t# fit on training data\n",
        "\t\ts.fit(longX)\n",
        "\t\t# apply to training and test data\n",
        "\t\tlongX = s.transform(longX)\n",
        "\t\tflatTrainX = s.transform(flatTrainX)\n",
        "\t\tflatTestX = s.transform(flatTestX)\n",
        "\t# reshape\n",
        "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
        "\tflatTestX = flatTestX.reshape((testX.shape))\n",
        "\treturn flatTrainX, flatTestX\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy, param):\n",
        "\tverbose, epochs, batch_size = 0, 10, 32\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\t# scale data\n",
        "\ttrainX, testX = scale_data(trainX, testX, param)\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(MaxPooling1D(pool_size=2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores, params):\n",
        "\tprint(scores, params)\n",
        "\t# summarize mean and standard deviation\n",
        "\tfor i in range(len(scores)):\n",
        "\t\tm, s = mean(scores[i]), std(scores[i])\n",
        "\t\tprint('Param=%s: %.3f%% (+/-%.3f)' % (params[i], m, s))\n",
        "\t# boxplot of scores\n",
        "\tpyplot.boxplot(scores, labels=params)\n",
        "\tpyplot.savefig('exp_cnn_standardize.png')\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(params, repeats=3):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# test each parameter\n",
        "\tall_scores = list()\n",
        "\tfor p in params:\n",
        "\t\t# repeat experiment\n",
        "\t\tscores = list()\n",
        "\t\tfor r in range(repeats):\n",
        "\t\t\tscore = evaluate_model(trainX, trainy, testX, testy, p)\n",
        "\t\t\tscore = score * 100.0\n",
        "\t\t\tprint('>p=%s #%d: %.3f' % (p, r+1, score))\n",
        "\t\t\tscores.append(score)\n",
        "\t\tall_scores.append(scores)\n",
        "\t# summarize results\n",
        "\tsummarize_results(all_scores, params)\n",
        "\n",
        "# run the experiment\n",
        "n_params = [False, True]\n",
        "run_experiment(n_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "HuYf5OVterFp",
        "outputId": "e4ce58a7-408a-4ec8-dbea-6deb7c85c8bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">p=False #1: 90.974\n",
            ">p=False #2: 89.515\n",
            ">p=False #3: 90.465\n",
            ">p=True #1: 89.345\n",
            ">p=True #2: 91.483\n",
            ">p=True #3: 91.144\n",
            "[[90.97387194633484, 89.51476216316223, 90.46487808227539], [89.34509754180908, 91.48286581039429, 91.14353656768799]] [False, True]\n",
            "Param=False: 90.318% (+/-0.605)\n",
            "Param=True: 90.657% (+/-0.938)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOY0lEQVR4nO3df6zddX3H8edLu4noNL39wRQpZcMlGBZUTjqVlc1pmGNGkKnzxyJLFGLCMtgyM5eYgFuygHE//lminRj7x3TBMadMIzR1QrZp5dbU0VKtjoHSqb2OMie4AeW9P+63SS339p62597bvvt8JCfnnu/5fs/5nHB4nm8/58c3VYUkqa+nLfcAJEmLy9BLUnOGXpKaM/SS1Jyhl6TmViz3AOayevXqWr9+/XIPQ5JOGtu3b/9+Va2Z67oTMvTr169nenp6uYchSSeNJA/Md51TN5LUnKGXpOYMvSQ1N1bok1ybZGeSXUmuG5a9cbj8ZJLREba9P8k9SXYkceJdkpbYgm/GJjkfuArYADwGfC7JPwI7gSuAD41xP6+squ8fz0AlScdmnD3684BtVfVoVT0B3AlcUVW7q+rrizs8SdLxGif0O4GNSVYlOR24FDjrKO6jgDuSbE9y9bEMUpJ07Bacuqmq3UluAu4AHgF2AAeO4j5+sar2JlkLbEnytaq66/CVhheBqwHWrVt3FDcvSTqSsd6Mraqbq+rCqroY2A/sGfcOqmrvcL4P+CSzc/1zrbepqkZVNVqzZs4vd0k6CSU5ppMmZ9xP3awdztcx+wbsx8bc7llJfurg38AlzE4FSTpFVNW8pyNdr8kZ93P0tya5F7gNuKaqHk7y+iQPAi8HPpPkdoAkz0/y2WG7M4B/TvJV4MvAZ6rqcxN+DJKkIxjrt26qauMcyz7J7FTM4cv/k9k3bKmq+4ALjnOMkqTj4DdjJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl7SRExNTR3Tb84fzfpTU1PL/ChPTmP9eqUkLWT//v2L/jvyHpDk2LhHL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6Tm/JliSRNR1z8Hbnju4t+HjpqhlzQRed8PluT36OuGRb2Llpy6kaTmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLU3FihT3Jtkp1JdiW5blj2xuHyk0lGR9j2NUm+nuSbSd4zqYFLksazYOiTnA9cBWwALgBem+RcYCdwBXDXEbZ9OvBXwK8BLwLekuRFExi3JGlM4+zRnwdsq6pHq+oJ4E7giqraXVVfX2DbDcA3q+q+qnoM+FvgsuMbsiTpaIwT+p3AxiSrkpwOXAqcNebtnwl8+5DLDw7LniLJ1Ummk0zPzMyMefOSpIUsGPqq2g3cBNwBfA7YARyY9ECqalNVjapqtGbNmknfvCSdssZ6M7aqbq6qC6vqYmA/sGfM29/Lj+/9v2BYpglLckwnSf2N9TPFSdZW1b4k65h9A/ZlY97+3cALk5zDbODfDLz1mEaqI5rv52GTLPpPx0o6sY37Ofpbk9wL3AZcU1UPJ3l9kgeBlwOfSXI7QJLnJ/kswPDm7e8AtwO7gVuqatfEH4UkaV45Eff2RqNRTU9PL/cwWnCPXktlKZ5rPp/nl2R7Vc35nSa/GStJzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/QnmampKZKMfQKOav0kTE1NLfOjlDRJK5Z7ADo6+/fvp6oW9T4OvkBI6sE9eklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqbmxQp/k2iQ7k+xKct2wbCrJliTfGM5XzrPtgSQ7htOnJzl4SdLCFgx9kvOBq4ANwAXAa5OcC7wH2FpVLwS2Dpfn8qOqevFwet2Exi1JGtM4e/TnAduq6tGqegK4E7gCuAzYPKyzGbh8cYYoSToe44R+J7AxyaokpwOXAmcBZ1TVd4Z1vgucMc/2pyWZTvKlJPO+GCS5elhvemZm5mgegyTpCBY8ZmxV7U5yE3AH8AiwAzhw2DqVZL4DmZ5dVXuT/Azw+ST3VNW/z3E/m4BNAKPRaHEPiipJp5Cx3oytqpur6sKquhjYD+wBvpfkeQDD+b55tt07nN8HfAF4yQTGLUka04J79ABJ1lbVviTrmJ2ffxlwDnAlcONw/qk5tlsJPFpV/5dkNXAR8P5JDf5UVNc/B2547uLfh6Q2xgo9cGuSVcDjwDVV9XCSG4FbkrwDeAB4E0CSEfCuqnons2/kfijJk8z+6+HGqrp34o/iFJL3/YCqxZ3ZSkLdsKh3IWkJjRX6qto4x7L/Al41x/Jp4J3D3/8K/PxxjlGSdBz8ZqwkNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpuXGPGStJC0qyqLe/cuXKRb39rgy9pIk4loPWJ1n0g93LqRtJas/QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWpurNAnuTbJziS7klw3LJtKsiXJN4bzOY/xleTKYZ1vJLlykoOXJC1swdAnOR+4CtgAXAC8Nsm5wHuArVX1QmDrcPnwbaeA64FfGLa/fr4XBEnS4hhnj/48YFtVPVpVTwB3AlcAlwGbh3U2A5fPse2vAluq6qGq2g9sAV5z/MOWJI1rnNDvBDYmWZXkdOBS4CzgjKr6zrDOd4Ez5tj2TODbh1x+cFj2FEmuTjKdZHpmZmbsByBJOrIVC61QVbuT3ATcATwC7AAOHLZOJTmuQ7lX1SZgE8BoNPKw8EeQZFFvf+VKZ9ekTsZ6M7aqbq6qC6vqYmA/sAf4XpLnAQzn++bYdC+ze/8HvWBYpmNUVUd1OpZtHnrooWV+lJImadxP3awdztcxOz//MeDTwMFP0VwJfGqOTW8HLkmycngT9pJhmSRpiSw4dTO4Nckq4HHgmqp6OMmNwC1J3gE8ALwJIMkIeFdVvbOqHkryJ8Ddw+38cVW5uyhJSygH/3l/IhmNRjU9Pb3cw2ghCSfif2MJfH5OUpLtVTWa6zq/GStJzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktTcWKFP8ntJdiXZmeTjSU5L8itJvjIs25xkxTzbHkiyYzh9erLDlyQtZMHQJzkT+F1gVFXnA08H3gpsBt48LHsAuHKem/hRVb14OL1uQuOWJI1p3KmbFcAzh73204FHgMeqas9w/RbgNxZhfBpTkjlPR7ru4PWSelsw9FW1F/gA8C3gO8B/A7cAK5KMhtXeAJw1z02clmQ6yZeSXD6BMWsOVXVMJ0n9jTN1sxK4DDgHeD7wLOBtwJuBv0jyZeB/gAPz3MTZVTVidrrnL5P87Dz3c/XwgjA9MzNz9I9EkjSncaZuXg38R1XNVNXjwN8Dr6iqL1bVxqraANwF7Jlr4+FfBFTVfcAXgJfMs96mqhpV1WjNmjXH8FAkSXMZJ/TfAl6W5PTMTuq+CtidZC1AkmcAfwh88PANk6wcrifJauAi4N5JDV6StLBx5ui3AX8HfAW4Z9hmE/DuJLuBfwNuq6rPAyQZJfnwsPl5wHSSrwL/BNxYVYZekpZQTsQ35EajUU1PTy/3MCQtsiR+KGBCkmwf3g99Cr8ZK0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc3NeVQoSZqUhY57MN/1fmN2cgy9pEVlsJefUzeS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpo7IY8Zm2QGeGC5x9HEauD7yz0IaR4+Pyfn7KpaM9cVJ2ToNTlJpuc7YLC03Hx+Lg2nbiSpOUMvSc0Z+v42LfcApCPw+bkEnKOXpObco5ek5gy9JDXngUdOQkkOAPccsujyqrp/nnV/WFXPXpKBSUCSVcDW4eJPAweAmeHyhqp6bFkGdgpzjv4kdDTxNvRaTkluAH5YVR84ZNmKqnpi+UZ16nHqpoEkz06yNclXktyT5LI51nlekruS7EiyM8nGYfklSb44bPuJJL4oaOKSfDTJB5NsA96f5IYkf3DI9TuTrB/+/q0kXx6eqx9K8vRlGnYbhv7k9Mzhf4IdST4J/C/w+qp6KfBK4M/y1CMuvxW4vapeDFwA7EiyGngv8Oph22ng95fuYegU8wLgFVU173MsyXnAbwIXDc/VA8Dblmh8bTlHf3L60fA/AQBJfgL40yQXA08CZwJnAN89ZJu7gY8M6/5DVe1I8kvAi4B/GV4XfhL44hI9Bp16PlFVBxZY51XAhcDdw3PymcC+xR5Yd4a+h7cBa4ALq+rxJPcDpx26QlXdNbwQ/Drw0SR/DuwHtlTVW5Z6wDolPXLI30/w4zMKB5+vATZX1R8t2ahOAU7d9PBcYN8Q+VcCZx++QpKzge9V1V8DHwZeCnwJuCjJucM6z0ryc0s4bp267mf2OUiSlwLnDMu3Am9Isna4bmp47uo4uEffw98AtyW5h9l59q/Nsc4vA+9O8jjwQ+DtVTWT5LeBjyd5xrDee4E9iz9kneJuBd6eZBewjeE5V1X3JnkvcEeSpwGPA9fgz5YfFz9eKUnNOXUjSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNff/5Co+dNVpYsEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6SZOYF0hbqm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Number of Filters"
      ],
      "metadata": {
        "id": "q_zFZMTQi2SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn model with filters\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy, n_filters):\n",
        "\tverbose, epochs, batch_size = 0, 10, 32\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(MaxPooling1D(pool_size=2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores, params):\n",
        "\tprint(scores, params)\n",
        "\t# summarize mean and standard deviation\n",
        "\tfor i in range(len(scores)):\n",
        "\t\tm, s = mean(scores[i]), std(scores[i])\n",
        "\t\tprint('Param=%d: %.3f%% (+/-%.3f)' % (params[i], m, s))\n",
        "\t# boxplot of scores\n",
        "\tpyplot.boxplot(scores, labels=params)\n",
        "\tpyplot.savefig('exp_cnn_filters.png')\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(params, repeats=3):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# test each parameter\n",
        "\tall_scores = list()\n",
        "\tfor p in params:\n",
        "\t\t# repeat experiment\n",
        "\t\tscores = list()\n",
        "\t\tfor r in range(repeats):\n",
        "\t\t\tscore = evaluate_model(trainX, trainy, testX, testy, p)\n",
        "\t\t\tscore = score * 100.0\n",
        "\t\t\tprint('>p=%d #%d: %.3f' % (p, r+1, score))\n",
        "\t\t\tscores.append(score)\n",
        "\t\tall_scores.append(scores)\n",
        "\t# summarize results\n",
        "\tsummarize_results(all_scores, params)\n",
        "\n",
        "# run the experiment\n",
        "n_params = [8, 16, 32, 64, 128, 256]\n",
        "run_experiment(n_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "LcCmuvFyi3u6",
        "outputId": "77b0c2a2-411e-4c3f-d90f-3396b1d327ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">p=8 #1: 89.854\n",
            ">p=8 #2: 88.022\n",
            ">p=8 #3: 90.261\n",
            ">p=16 #1: 89.820\n",
            ">p=16 #2: 90.465\n",
            ">p=16 #3: 89.243\n",
            ">p=32 #1: 88.429\n",
            ">p=32 #2: 90.058\n",
            ">p=32 #3: 91.279\n",
            ">p=64 #1: 92.229\n",
            ">p=64 #2: 91.279\n",
            ">p=64 #3: 91.313\n",
            ">p=128 #1: 89.684\n",
            ">p=128 #2: 90.940\n",
            ">p=128 #3: 90.635\n",
            ">p=256 #1: 89.311\n",
            ">p=256 #2: 91.347\n",
            ">p=256 #3: 90.736\n",
            "[[89.85409140586853, 88.021719455719, 90.26128053665161], [89.8201584815979, 90.46487808227539, 89.24329876899719], [88.42890858650208, 90.05768299102783, 91.27926826477051], [92.22938418388367, 91.27926826477051, 91.31320118904114], [89.68442678451538, 90.93993902206421, 90.63454270362854], [89.31116461753845, 91.34713411331177, 90.73634147644043]] [8, 16, 32, 64, 128, 256]\n",
            "Param=8: 89.379% (+/-0.974)\n",
            "Param=16: 89.843% (+/-0.499)\n",
            "Param=32: 89.922% (+/-1.168)\n",
            "Param=64: 91.607% (+/-0.440)\n",
            "Param=128: 90.420% (+/-0.535)\n",
            "Param=256: 90.465% (+/-0.853)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Size of Kernel\n",
        "The size of the kernel is another important hyperparameter of the 1D CNN to tune.\n",
        "\n",
        "The kernel size controls the number of time steps consider in each “read” of the input sequence, that is then projected onto the feature map (via the convolutional process).\n",
        "\n",
        "A large kernel size means a less rigorous reading of the data, but may result in a more generalized snapshot of the input."
      ],
      "metadata": {
        "id": "06rFw30ljMCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn model vary kernel size\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy, n_kernel):\n",
        "\tverbose, epochs, batch_size = 0, 15, 32\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=n_kernel, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=n_kernel, activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(MaxPooling1D(pool_size=2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores, params):\n",
        "\tprint(scores, params)\n",
        "\t# summarize mean and standard deviation\n",
        "\tfor i in range(len(scores)):\n",
        "\t\tm, s = mean(scores[i]), std(scores[i])\n",
        "\t\tprint('Param=%d: %.3f%% (+/-%.3f)' % (params[i], m, s))\n",
        "\t# boxplot of scores\n",
        "\tpyplot.boxplot(scores, labels=params)\n",
        "\tpyplot.savefig('exp_cnn_kernel.png')\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(params, repeats=3):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# test each parameter\n",
        "\tall_scores = list()\n",
        "\tfor p in params:\n",
        "\t\t# repeat experiment\n",
        "\t\tscores = list()\n",
        "\t\tfor r in range(repeats):\n",
        "\t\t\tscore = evaluate_model(trainX, trainy, testX, testy, p)\n",
        "\t\t\tscore = score * 100.0\n",
        "\t\t\tprint('>p=%d #%d: %.3f' % (p, r+1, score))\n",
        "\t\t\tscores.append(score)\n",
        "\t\tall_scores.append(scores)\n",
        "\t# summarize results\n",
        "\tsummarize_results(all_scores, params)\n",
        "\n",
        "# run the experiment\n",
        "n_params = [2, 3, 5, 7, 11]\n",
        "run_experiment(n_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "g12tRaMJjNRJ",
        "outputId": "a28f6ec8-14aa-4923-bad6-2f178e194933"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">p=2 #1: 88.734\n",
            ">p=2 #2: 89.650\n",
            ">p=2 #3: 87.716\n",
            ">p=3 #1: 91.720\n",
            ">p=3 #2: 90.635\n",
            ">p=3 #3: 91.279\n",
            ">p=5 #1: 90.702\n",
            ">p=5 #2: 87.275\n",
            ">p=5 #3: 90.499\n",
            ">p=7 #1: 92.263\n",
            ">p=7 #2: 92.433\n",
            ">p=7 #3: 91.754\n",
            ">p=11 #1: 91.856\n",
            ">p=11 #2: 93.247\n",
            ">p=11 #3: 91.211\n",
            "[[88.73430490493774, 89.65049386024475, 87.71632313728333], [91.7203962802887, 90.63454270362854, 91.27926826477051], [90.7024085521698, 87.27519512176514, 90.49881100654602], [92.2633171081543, 92.43298172950745, 91.75432920455933], [91.85612201690674, 93.24737191200256, 91.21140241622925]] [2, 3, 5, 7, 11]\n",
            "Param=2: 88.700% (+/-0.790)\n",
            "Param=3: 91.211% (+/-0.446)\n",
            "Param=5: 89.492% (+/-1.570)\n",
            "Param=7: 92.150% (+/-0.288)\n",
            "Param=11: 92.105% (+/-0.850)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GmnFgZkjS82"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Headed Convolutional Neural Network\n",
        "Another popular approach with CNNs is to have a multi-headed model, where each head of the model reads the input time steps using a different sized kernel.\n",
        "\n",
        "For example, a three-headed model may have three different kernel sizes of 3, 5, 11, allowing the model to read and interpret the sequence data at three different resolutions. The interpretations from all three heads are then concatenated within the model and interpreted by a fully-connected layer before a prediction is made."
      ],
      "metadata": {
        "id": "p-LZ2d5kjZ17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-headed cnn model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import concatenate\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\tverbose, epochs, batch_size = 0, 10, 32\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        " \t# head 1\n",
        "\tinputs1 = Input(shape=(n_timesteps,n_features))\n",
        "\tconv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# head 2\n",
        "\tinputs2 = Input(shape=(n_timesteps,n_features))\n",
        "\tconv2 = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# head 3\n",
        "\tinputs3 = Input(shape=(n_timesteps,n_features))\n",
        "\tconv3 = Conv1D(filters=64, kernel_size=11, activation='relu')(inputs3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(100, activation='relu')(merged)\n",
        "\toutputs = Dense(n_outputs, activation='softmax')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# save a plot of the model\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit([trainX,trainX,trainX], trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate([testX,testX,testX], testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# repeat experiment\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
        "\t\tscore = score * 100.0\n",
        "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\t\tscores.append(score)\n",
        "\t# summarize results\n",
        "\tsummarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQJ-IXOsjbZZ",
        "outputId": "595b2117-d0cf-4965-f83a-7058333018de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">#1: 91.992\n",
            ">#2: 92.637\n",
            ">#3: 91.313\n",
            "[91.99185371398926, 92.63657927513123, 91.31320118904114]\n",
            "Accuracy: 91.981% (+/-0.540)\n"
          ]
        }
      ]
    }
  ]
}