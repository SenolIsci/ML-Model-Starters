{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/SenolIsci/ML-Model-Starters/blob/main/CNN_Models_for_Human_Activity_Recognition.ipynb",
      "authorship_tag": "ABX9TyO3IJ5lnlkHLR40MpBZNDE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SenolIsci/ML-Model-Starters/blob/main/LSTM_Models_for_Human_Activity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Activity Recognition Using Smartphones Dataset\n",
        "\n",
        "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
        "\n",
        "Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field.\n",
        "\n",
        "Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data.\n",
        "\n",
        "A standard human activity recognition dataset is the ‘Activity Recognition Using Smart Phones Dataset’ made available in 2012.\n",
        "\n",
        "The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository:\n",
        "\n",
        "[Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)\n",
        "\n",
        "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos.\n",
        "\n",
        "1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "by Jason Brownlee on September 21, 2018 in Deep Learning for Time Series\n",
        "Tweet Tweet  Share\n",
        "Last Updated on August 28, 2020\n",
        "\n",
        "Human activity recognition is the problem of classifying sequences of accelerometer data recorded by specialized harnesses or smart phones into known well-defined movements.\n",
        "\n",
        "Classical approaches to the problem involve hand crafting features from the time series data based on fixed-sized windows and training machine learning models, such as ensembles of decision trees. The difficulty is that this feature engineering requires deep expertise in the field.\n",
        "\n",
        "Recently, deep learning methods such as recurrent neural networks and one-dimensional convolutional neural networks, or CNNs, have been shown to provide state-of-the-art results on challenging activity recognition tasks with little or no data feature engineering, instead using feature learning on raw data.\n",
        "\n",
        "In this tutorial, you will discover how to develop one-dimensional convolutional neural networks for time series classification on the problem of human activity recognition.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "How to load and prepare the data for a standard human activity recognition dataset and develop a single 1D CNN model that achieves excellent performance on the raw data.\n",
        "How to further tune the performance of the model, including data transformation, filter maps, and kernel sizes.\n",
        "How to develop a sophisticated multi-headed one-dimensional convolutional neural network model that provides an ensemble-like result.\n",
        "Kick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n",
        "\n",
        "Let’s get started.\n",
        "\n",
        "How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition\n",
        "Photo by Wolfgang Staudt, some rights reserved.\n",
        "\n",
        "Tutorial Overview\n",
        "This tutorial is divided into four parts; they are:\n",
        "\n",
        "Activity Recognition Using Smartphones Dataset\n",
        "Develop 1D Convolutional Neural Network\n",
        "Tuned 1D Convolutional Neural Network\n",
        "Multi-Headed 1D Convolutional Neural Network\n",
        "\n",
        "Activity Recognition Using Smartphones Dataset\n",
        "Human Activity Recognition, or HAR for short, is the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
        "\n",
        "A standard human activity recognition dataset is the ‘Activity Recognition Using Smart Phones Dataset’ made available in 2012.\n",
        "\n",
        "It was prepared and made available by Davide Anguita, et al. from the University of Genova, Italy and is described in full in their 2013 paper “A Public Domain Dataset for Human Activity Recognition Using Smartphones.” The dataset was modeled with machine learning algorithms in their 2012 paper titled “Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine.”\n",
        "\n",
        "The dataset was made available and can be downloaded for free from the UCI Machine Learning Repository:\n",
        "\n",
        "Human Activity Recognition Using Smartphones Data Set, UCI Machine Learning Repository\n",
        "The data was collected from 30 subjects aged between 19 and 48 years old performing one of six standard activities while wearing a waist-mounted smartphone that recorded the movement data. Video was recorded of each subject performing the activities and the movement data was labeled manually from these videos.\n",
        "\n",
        "Below is an example video of a subject performing the activities while their movement data is being recorded.\n",
        "\n",
        "\n",
        "The six activities performed were as follows:\n",
        "\n",
        "Walking\n",
        "Walking Upstairs\n",
        "Walking Downstairs\n",
        "Sitting\n",
        "Standing\n",
        "Laying\n",
        "The movement data recorded was the x, y, and z accelerometer data (linear acceleration) and gyroscopic data (angular velocity) from the smart phone, specifically a Samsung Galaxy S II. Observations were recorded at 50 Hz (i.e. 50 data points per second). Each subject performed the sequence of activities twice, once with the device on their left-hand-side and once with the device on their right-hand side.\n",
        "\n",
        "The raw data is not available. Instead, a pre-processed version of the dataset was made available. The pre-processing steps included:\n",
        "\n",
        "Pre-processing accelerometer and gyroscope using noise filters.\n",
        "Splitting data into fixed windows of 2.56 seconds (128 data points) with 50% overlap.\n",
        "Splitting of accelerometer data into gravitational (total) and body motion components.\n",
        "Feature engineering was applied to the window data, and a copy of the data with these engineered features was made available.\n",
        "\n",
        "A number of time and frequency features commonly used in the field of human activity recognition were extracted from each window. The result was a 561 element vector of features.\n",
        "\n",
        "The dataset was split into train (70%) and test (30%) sets based on data for subjects, e.g. 21 subjects for train and nine for test.\n",
        "\n",
        "There are three main signal types in the raw data: total acceleration, body acceleration, and body gyroscope. Each has three axes of data. This means that there are a total of nine variables for each time step.\n",
        "\n",
        "Further, each series of data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps. These windows of data correspond to the windows of engineered features (rows) in the previous section.\n",
        "\n",
        "The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. \n",
        "\n",
        "To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file.\n"
      ],
      "metadata": {
        "id": "UQKFCcEkc-A2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Y216Z8zhxfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lqu-8fwRhxjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Develop an LSTM Network Model"
      ],
      "metadata": {
        "id": "aTUZD-R5gQxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        " dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        " return dataframe.values\n",
        " \n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        " loaded = list()\n",
        " for name in filenames:\n",
        "   data = load_file(prefix + name)\n",
        "   loaded.append(data)\n",
        " # stack group so that features are the 3rd dimension\n",
        " loaded = dstack(loaded)\n",
        " return loaded\n",
        " \n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        " filepath = prefix + group + '/Inertial Signals/'\n",
        " # load all 9 files as a single array\n",
        " filenames = list()\n",
        " # total acceleration\n",
        " filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        " # body acceleration\n",
        " filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        " # body gyroscope\n",
        " filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        " # load input data\n",
        " X = load_group(filenames, filepath)\n",
        " # load class output\n",
        " y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        " return X, y\n",
        " \n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        " # load all train\n",
        " trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        " print(trainX.shape, trainy.shape)\n",
        " # load all test\n",
        " testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        " print(testX.shape, testy.shape)\n",
        " # zero-offset class values\n",
        " trainy = trainy - 1\n",
        " testy = testy - 1\n",
        " # one hot encode y\n",
        " trainy = to_categorical(trainy)\n",
        " testy = to_categorical(testy)\n",
        " print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        " return trainX, trainy, testX, testy\n",
        " \n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        " verbose, epochs, batch_size = 0, 15, 64\n",
        " n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        " model = Sequential()\n",
        " model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
        " model.add(Dropout(0.5))\n",
        " model.add(Dense(100, activation='relu'))\n",
        " model.add(Dense(n_outputs, activation='softmax'))\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " # fit network\n",
        " model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        " # evaluate model\n",
        " _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        " return accuracy\n",
        " \n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        " print(scores)\n",
        " m, s = mean(scores), std(scores)\n",
        " print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        " \n",
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        " # load data\n",
        " trainX, trainy, testX, testy = load_dataset()\n",
        " # repeat experiment\n",
        " scores = list()\n",
        " for r in range(repeats):\n",
        "  score = evaluate_model(trainX, trainy, testX, testy)\n",
        "  score = score * 100.0\n",
        " print('>#%d: %.3f' % (r+1, score))\n",
        " scores.append(score)\n",
        " # summarize results\n",
        " summarize_results(scores)\n",
        " \n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "metadata": {
        "id": "t4Z2XTq-pYUM",
        "outputId": "9694d37b-34dd-4cf6-c97b-4b8025a0ecdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">#3: 90.159\n",
            "[90.15948176383972]\n",
            "Accuracy: 90.159% (+/-0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Develop a CNN-LSTM Network Model\n",
        "The CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.\n",
        "\n",
        "CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images (e.g. videos). Specifically, the problems of:\n",
        "\n",
        "* Activity Recognition: Generating a textual description of an activity demonstrated in a sequence of images.\n",
        "* Image Description: Generating a textual description of a single image.\n",
        "* Video Description: Generating a textual description of a sequence of images.\n",
        "\n",
        "The CNN LSTM model will read subsequences of the main sequence in as blocks, extract features from each block, then allow the LSTM to interpret the features extracted from each block.\n",
        "\n",
        "One approach to implementing this model is to split each window of 128 time steps into subsequences for the CNN model to process. For example, the 128 time steps in each window can be split into four subsequences of 32 time steps.\n",
        "\n",
        "The entire CNN model can be wrapped in a TimeDistributed layer to allow the same CNN model to read in each of the four subsequences in the window.\n",
        "We can think of the framing of the problem with time steps and a TimeDistributed layer as a more compact way of implementing without framing. It may even be more efficient (space,parameters or time wise) at a larger scale.It does not affect the accuracy.\n",
        "\n",
        "\n",
        " The extracted features are then flattened and provided to the LSTM model to read, extracting its own features before a final mapping to an activity is made.\n",
        "\n",
        "It is common to use two consecutive CNN layers followed by dropout and a max pooling layer, and that is the simple structure used in the CNN LSTM model here."
      ],
      "metadata": {
        "id": "NFs1toHxhIt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn lstm model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        " dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        " return dataframe.values\n",
        " \n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        " loaded = list()\n",
        " for name in filenames:\n",
        "  data = load_file(prefix + name)\n",
        "  loaded.append(data)\n",
        " # stack group so that features are the 3rd dimension\n",
        " loaded = dstack(loaded)\n",
        " return loaded\n",
        " \n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        " filepath = prefix + group + '/Inertial Signals/'\n",
        " # load all 9 files as a single array\n",
        " filenames = list()\n",
        " # total acceleration\n",
        " filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        " # body acceleration\n",
        " filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        " # body gyroscope\n",
        " filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        " # load input data\n",
        " X = load_group(filenames, filepath)\n",
        " # load class output\n",
        " y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        " return X, y\n",
        " \n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        " # load all train\n",
        " trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        " print(trainX.shape, trainy.shape)\n",
        " # load all test\n",
        " testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        " print(testX.shape, testy.shape)\n",
        " # zero-offset class values\n",
        " trainy = trainy - 1\n",
        " testy = testy - 1\n",
        " # one hot encode y\n",
        " trainy = to_categorical(trainy)\n",
        " testy = to_categorical(testy)\n",
        " print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        " return trainX, trainy, testX, testy\n",
        " \n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        " # define model\n",
        " verbose, epochs, batch_size = 0, 25, 64\n",
        " n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        " # reshape data into time steps of sub-sequences\n",
        " n_steps, n_length = 4, 32\n",
        " trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
        " testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
        " # define model\n",
        " model = Sequential()\n",
        " model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        " model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        " model.add(TimeDistributed(Dropout(0.5)))\n",
        " model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        " model.add(TimeDistributed(Flatten()))\n",
        " model.add(LSTM(100))\n",
        " model.add(Dropout(0.5))\n",
        " model.add(Dense(100, activation='relu'))\n",
        " model.add(Dense(n_outputs, activation='softmax'))\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " # fit network\n",
        " model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        " # evaluate model\n",
        " _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        " return accuracy\n",
        " \n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        " print(scores)\n",
        " m, s = mean(scores), std(scores)\n",
        " print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        " \n",
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        " # load data\n",
        " trainX, trainy, testX, testy = load_dataset()\n",
        " # repeat experiment\n",
        " scores = list()\n",
        " for r in range(repeats):\n",
        "  score = evaluate_model(trainX, trainy, testX, testy)\n",
        "  score = score * 100.0\n",
        " print('>#%d: %.3f' % (r+1, score))\n",
        " scores.append(score)\n",
        " # summarize results\n",
        " summarize_results(scores)\n",
        " \n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "metadata": {
        "id": "ev1z9JCNg6tE",
        "outputId": "f124656b-37a3-4cd7-ad5c-ac37f770b210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">#3: 89.820\n",
            "[89.8201584815979]\n",
            "Accuracy: 89.820% (+/-0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Develop a ConvLSTM Network Model\n",
        "A further extension of the CNN LSTM idea is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM.\n",
        "\n",
        "This combination is called a Convolutional LSTM, or ConvLSTM for short, and like the CNN LSTM is also used for spatio-temporal data.\n",
        "\n",
        "Unlike an LSTM that reads the data in directly in order to calculate internal state and state transitions, and unlike the CNN LSTM that is interpreting the output from CNN models, the ConvLSTM is using convolutions directly as part of reading input into the LSTM units themselves.\n",
        "\n",
        "he ConvLSTM2D class, by default, expects input data to have the shape:\n",
        "```\n",
        "(samples, time, rows, cols, channels)\n",
        "```\n",
        "\n",
        "For this chosen framing of the problem, the input for the ConvLSTM2D would therefore be:\n",
        "\n",
        "* Samples: n, for the number of windows in the dataset.\n",
        "* Time: 4, for the four subsequences that we split a window of 128 time steps into.\n",
        "* Rows: 1, for the one-dimensional shape of each subsequence.\n",
        "* Columns: 32, for the 32 time steps in an input subsequence.\n",
        "* Channels: 9, for the nine input variables."
      ],
      "metadata": {
        "id": "hW0qYXIUtbTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convlstm model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "\tfilepath = prefix + group + '/Inertial Signals/'\n",
        "\t# load all 9 files as a single array\n",
        "\tfilenames = list()\n",
        "\t# total acceleration\n",
        "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "\t# body acceleration\n",
        "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "\t# body gyroscope\n",
        "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "\t# load input data\n",
        "\tX = load_group(filenames, filepath)\n",
        "\t# load class output\n",
        "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "\treturn X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "\t# load all train\n",
        "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        "\tprint(trainX.shape, trainy.shape)\n",
        "\t# load all test\n",
        "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        "\tprint(testX.shape, testy.shape)\n",
        "\t# zero-offset class values\n",
        "\ttrainy = trainy - 1\n",
        "\ttesty = testy - 1\n",
        "\t# one hot encode y\n",
        "\ttrainy = to_categorical(trainy)\n",
        "\ttesty = to_categorical(testy)\n",
        "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\treturn trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# define model\n",
        "\tverbose, epochs, batch_size = 0, 25, 64\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\t# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "\tn_steps, n_length = 4, 32\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "\ttestX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "\t# load data\n",
        "\ttrainX, trainy, testX, testy = load_dataset()\n",
        "\t# repeat experiment\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
        "\t\tscore = score * 100.0\n",
        "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\t\tscores.append(score)\n",
        "\t# summarize results\n",
        "\tsummarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "metadata": {
        "id": "JBG33sFup44S",
        "outputId": "a6d73332-61f5-43c8-9549-2b2b4fa0f20a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            ">#1: 90.567\n",
            ">#2: 91.110\n",
            ">#3: 89.481\n",
            "[90.56667685508728, 91.10960364341736, 89.4808292388916]\n",
            "Accuracy: 90.386% (+/-0.677)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jrYbwfZuiSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}